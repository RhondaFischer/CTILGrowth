{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5014c0d3",
   "metadata": {},
   "source": [
    "# Script for parsing XML files \n",
    "\n",
    "Files exported from IU One Search, read into this script, leading to an export of a graph.gexf file saved to drive for reading into Gephi for visualization and Louvain Modularity grouping. \n",
    "\n",
    "R. Fischer\n",
    "rkfische@iu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e2a26",
   "metadata": {},
   "source": [
    "## packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d102ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import networkx as nx\n",
    "#import community.community_louvain\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864397d2",
   "metadata": {},
   "source": [
    "## Microbiome exports with second tier search for bacteria in the subject lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165764ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nxml_files = [\"microbiome_amplicon.xml\", \"microbiome_wholegenome.xml\", \"microbiome_deNovo.xml\", \\n             \"microbiome_index.xml\",\\'microbiome_consortium.xml\\',\\n             \"microbiome_parkinson.xml\", \"microbiome_cancer.xml\", \\n             \"microbiome_diabetes.xml\", \"microbiome_alzheimer.xml\",\\n             \"ibs.xml\"]\\nfile_tags = [\\'amp\\', \\'wg\\', \\'dn\\', \\'ind\\', \\'con\\',\\'p\\', \\'c\\', \\'d\\', \\'a\\',\\'ib\\']\\n\\nbacteria_list = [\\'escherichia\\', \\'coli\\', \\'staphylococcus\\', \\'aureus\\',\\n                 \\'enterococcus\\', \\'faecalis\\',\\n                \\'staphylococcus\\', \\'epidermidis\\', \\'carbapenems\\',\\n                \\'salmonella\\', \\'lactobacillus\\', \\'streptococcus\\',\\n                \\'prevotella\\', \\'coxiella\\', \\'burnetii\\', \\'legionella\\',\\n                \\'akkermansia\\', \\'bifidobacterium\\',\\n                \\'lactobacillus\\', \\'escherichia\\', \\n                \\'roseburia\\', \\'blautia\\', \\'faecalibacterium\\']\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "xml_files = [\"microbiome_amplicon.xml\", \"microbiome_wholegenome.xml\", \"microbiome_deNovo.xml\", \n",
    "             \"microbiome_index.xml\",'microbiome_consortium.xml',\n",
    "             \"microbiome_parkinson.xml\", \"microbiome_cancer.xml\", \n",
    "             \"microbiome_diabetes.xml\", \"microbiome_alzheimer.xml\",\n",
    "             \"ibs.xml\"]\n",
    "file_tags = ['amp', 'wg', 'dn', 'ind', 'con','p', 'c', 'd', 'a','ib']\n",
    "\n",
    "bacteria_list = ['escherichia', 'coli', 'staphylococcus', 'aureus',\n",
    "                 'enterococcus', 'faecalis',\n",
    "                'staphylococcus', 'epidermidis', 'carbapenems',\n",
    "                'salmonella', 'lactobacillus', 'streptococcus',\n",
    "                'prevotella', 'coxiella', 'burnetii', 'legionella',\n",
    "                'akkermansia', 'bifidobacterium',\n",
    "                'lactobacillus', 'escherichia', \n",
    "                'roseburia', 'blautia', 'faecalibacterium']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba1f8d",
   "metadata": {},
   "source": [
    "## Tele-critical care search output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b9c2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_files = [\"tcc.xml\"]\n",
    "file_tags = ['t']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2e5e6",
   "metadata": {},
   "source": [
    "## Functions for parsing the xml file, creating records lists, tokenizing abstract and creating network graph file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b03f630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_root(file):\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d50de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def records_no_sub_subj(root):\n",
    "    text_list = []\n",
    "    record_list = []\n",
    "    article_count = 0\n",
    "    for record in root.findall(\"rec\"):\n",
    "        article_count +=1\n",
    "        record_list.append(record.attrib['resultID'])\n",
    "    # print(record_list)\n",
    "    return [record_list, article_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "105950ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def records_with_bacteria_subj(root):\n",
    "    text_list = []\n",
    "    record_list = []\n",
    "    article_count = 0\n",
    "    for record in root.findall(\"rec\"):\n",
    "        article_count +=1\n",
    "        try:\n",
    "            subjects = record.findall(\"header/controlInfo/artinfo/sug/\")\n",
    "            bacteria_flag = 0\n",
    "            if len(subjects)>0:\n",
    "                for subj in subjects:\n",
    "                    try:\n",
    "                        text_list.append(subj.text.lower().split())\n",
    "                        text_words = text_list.pop()\n",
    "                    except:\n",
    "                        pass\n",
    "                    for word in text_words:\n",
    "                        if word in bacteria_list:\n",
    "                            # print(\"word matches\", word)\n",
    "                            bacteria_flag = 1\n",
    "            if bacteria_flag == 1:\n",
    "                record_list.append(record.attrib['resultID'])\n",
    "        except:\n",
    "            pass\n",
    "    # print(record_list)\n",
    "    return [record_list, article_count]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74c15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tag(find_code, record_list, root, tag, G, flag=0 ):   \n",
    "    info_list=[]\n",
    "    rec_list =[]\n",
    "    \n",
    "    info=0\n",
    "    rec=0\n",
    "    for record in root.findall(\"rec\"):\n",
    "        rec+=1\n",
    "        # Check if ID includes a bacteria from record_list\n",
    "        if record.attrib['resultID'] in record_list:\n",
    "            recID = tag+record.attrib['resultID']\n",
    "            G.add_node(recID, type='recordID') \n",
    "            rec_list.append(recID)\n",
    "            #rec_list.append(tag)\n",
    "            tag_text = \"NAB\"\n",
    "            temp_list = []\n",
    "            if flag > 0:\n",
    "                try:\n",
    "                    temp_tag = record.findall(find_code)                \n",
    "                    if len(temp_tag)>0:\n",
    "                        for tt in temp_tag:\n",
    "                            try:\n",
    "                                if flag >1:\n",
    "                                    temp_list.append(tt.text.lower()[0:30])\n",
    "                                else:\n",
    "                                    sub_words = tt.text.lower().split()\n",
    "                                    for word in sub_words:\n",
    "                                        temp_list.append(word)\n",
    "                                while len(temp_list)>0:\n",
    "                                    tag_text = temp_list.pop()\n",
    "                                #print(tag_text)\n",
    "                            except:\n",
    "                                pass\n",
    "                    \n",
    "                    \n",
    "                except:\n",
    "                    tag_text = \"NAT\"\n",
    "                \n",
    "            else:\n",
    "                try:\n",
    "                    tag_text = record.find(find_code).attrib['year']\n",
    "                except:\n",
    "                    tag_text = \"NAT\"\n",
    "\n",
    "            info_list.append(tag_text)\n",
    "            if not tag_text.startswith(\"NA\"):\n",
    "                G.add_edge(recID, tag_text)\n",
    "    return info_list, rec_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "243ab9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_graph(node1,node2,label1, label2):\n",
    "    for n1, n2 in zip(node1,node2): \n",
    "        if n1 != \"NAT\": G.add_node(n1, type=label1) \n",
    "        if n2 != \"NAT\": G.add_node(n2,type=label2) \n",
    "        if n1 != \"NAT\" and n2 != \"NAT\": G.add_edge(n1, n2)\n",
    "\n",
    "def create_edge(list1, list2):\n",
    "    for l1 in list1:\n",
    "        for l2 in list2:\n",
    "            G.add_edge(l1,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb3e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstracts(record_list, root):\n",
    "    tag_dict = {'au':'author', 'atl': 'title', 'subj': 'subject', 'affil': 'affiliation',\n",
    "               'fmt': 'format'}\n",
    "    ab_text_list = []\n",
    "    rec_nbr=[]\n",
    "    for record in root.findall(\"rec\"):\n",
    "        if record.attrib['resultID'] in record_list:\n",
    "            try:\n",
    "                abstract = record.find(\"header/controlInfo/artinfo/ab\").text\n",
    "                #There may be more than one ab per article\n",
    "                if abstract not in ab_text_list:\n",
    "                    ab_text_list.append(abstract)\n",
    "                    rec_nbr.append(record.attrib['resultID'])\n",
    "            except:\n",
    "                pass\n",
    "    # print(rec_nbr)\n",
    "    return [ab_text_list, rec_nbr]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28f45865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_abstract_words(text_list, tag, rec_nbr):\n",
    "    topic_list = []\n",
    "    search_list = []\n",
    "    '''\n",
    "    vect = TfidfVectorizer(smooth_idf=True, sublinear_tf=False, norm=None, analyzer='word',\n",
    "                           max_features=3000, min_df=2, ngram_range=(1,1),\n",
    "                          stop_words='english', strip_accents ='ascii')\n",
    "    '''\n",
    "    vect = CountVectorizer(analyzer='word',\n",
    "                           max_features=100, min_df=2, ngram_range=(1,1),\n",
    "                          stop_words='english', strip_accents ='ascii')\n",
    "    X = vect.fit_transform(text_list)\n",
    "    feature_names = np.array(vect.get_feature_names())\n",
    "    # merge the search file tag to the record number to link back to a specific article\n",
    "    index_rec = [tag+str(x) for x in rec_nbr]\n",
    "    # save X array into a dataframe \n",
    "    # with word tokens as columns and file record number as row index\n",
    "    tf_df = pd.DataFrame(X.toarray(), columns = feature_names, index=index_rec)\n",
    "\n",
    "    return tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8330392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_generation(text_list, tag, rec_nbr, number_of_topics, number_of_tokens):\n",
    "    topic_list = []\n",
    "    search_list = []\n",
    "    '''\n",
    "    vect = TfidfVectorizer(smooth_idf=True, sublinear_tf=False, norm=None, analyzer='word',\n",
    "                           max_features=3000, min_df=2, ngram_range=(1,1),\n",
    "                          stop_words='english', strip_accents ='ascii')\n",
    "    '''\n",
    "    vect = CountVectorizer(analyzer='word',\n",
    "                           max_features=250, min_df=2, ngram_range=(1,1),\n",
    "                          stop_words='english', strip_accents ='ascii')\n",
    "    X = vect.fit_transform(text_list)\n",
    "    feature_names = np.array(vect.get_feature_names())\n",
    "    # merge the search file tag to the record number to link back to a specific article\n",
    "    index_rec = [tag+str(x) for x in rec_nbr]\n",
    "    # save X array into a dataframe \n",
    "    # with word tokens as columns and file record number as row index\n",
    "    tf_idf_df = pd.DataFrame(X.toarray(), columns = feature_names, index=index_rec)\n",
    "    print(tf_idf_df.columns)\n",
    "    \n",
    "    # count the nonzero values as part of the number of topics calculation\n",
    "    x_ar = X.todense()\n",
    "    non_zero_values = 0\n",
    "    for x in x_ar:\n",
    "        non_zero_values += np.count_nonzero(x)\n",
    "\n",
    "    #print(\"Number of abstract texts (length of text_list)\",len(text_list))\n",
    "    #print(\"number of words (len of feature_names)\", len(feature_names))\n",
    "    #print(\"non_zero_values\", non_zero_values)\n",
    "    number_of_topics = round( (len(text_list)*len(feature_names))/non_zero_values )\n",
    "    #print(\"Number of recommended topics:\", number_of_topics )\n",
    "\n",
    "    nmf = NMF(n_components=number_of_topics, solver=\"mu\")\n",
    "    W = nmf.fit_transform(X)\n",
    "    H = nmf.components_\n",
    "    for i, topic in enumerate(H):\n",
    "        topic_words=[]\n",
    "        for x in feature_names[topic.argsort()[-number_of_tokens:]]:\n",
    "            topic_words.append(x)\n",
    "        topic_list.append(topic_words)\n",
    "        search_list.append(tag+str(i+1))\n",
    "    return topic_list, search_list, tf_idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe2a65",
   "metadata": {},
   "source": [
    "## Main script calling the functions above\n",
    "\n",
    "Example of hacking vs coding is commenting out the tags to limit what's exported to the graph file.  Final product is network graph \"graph.gefx\" that's exported for reading into Gephi for visualizations and modularity grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7bf4f3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tcc.xml tag t\n",
      "['1', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '14', '16', '18', '19', '20', '21', '22', '23', '24', '25', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '52', '55', '59', '62', '63', '64', '65', '72']\n"
     ]
    }
   ],
   "source": [
    "#df = pd.DataFrame(columns = ['authors', 'recs1', 'journals', 'recs2','year', 'recs3', 'subjects', 'recs4', 'place', 'recs5'])\n",
    "G = nx.DiGraph()\n",
    "sub_subj_flag = 0\n",
    "\n",
    "for file, tag in zip(xml_files, file_tags):\n",
    "    # for linking results to the search tag vs individual articles\n",
    "    # G.add_node(tag, type='keyword')\n",
    "    \n",
    "    print(\"\\n\", file, \"tag\", tag)\n",
    "    # parse the file\n",
    "    root = create_root(file)\n",
    "    \n",
    "    # flag for using a two-tiered search, default is single tier\n",
    "    if sub_subj_flag > 0:\n",
    "        record_list, article_count = records_with_bacteria_subj(root)\n",
    "    else:\n",
    "        record_list, article_count = records_no_sub_subj(root)\n",
    "    \n",
    "    # for each abstract file within the search topic\n",
    "    ab_text_list, rec_nbr = get_abstracts(record_list, root)\n",
    "    print(rec_nbr)\n",
    "    tf_df = top_abstract_words(ab_text_list, tag, rec_nbr)\n",
    "    \n",
    "    for idx, row in tf_df.iterrows():\n",
    "        for i in range(len(row)):\n",
    "            if row[i] > 0 :\n",
    "                #print(row.index[i]) # get the column name word\n",
    "                G.add_node(row.index[i], type='topic')\n",
    "                G.add_edge(idx,row.index[i] )\n",
    "    \n",
    "    for record in record_list:\n",
    "        \n",
    "        # get lists of NMF topics, articles and words in dataframe matrix\n",
    "\n",
    "        \n",
    "        # get infor from the desired article components, authors, subjects, year, \n",
    "        \n",
    "        #authors, recs1 = get_tag(\".//aug\", record_list, root, tag)\n",
    "        #authors, recs1 = get_tag(\"header/controlInfo/artinfo/aug/\", record, root, tag, G, flag = 2)\n",
    "\n",
    "        #create_graph(authors, recs1, \"author\", \"rec\")\n",
    "\n",
    "        #journals, recs2 = get_tag(\".//jtl\", record, root, tag, G, flag=2)\n",
    "        #create_graph(journals, recs2, \"journal\", \"rec\")\n",
    "\n",
    "        #year, recs3 = get_tag(\".//dt\", record, root, tag, G, flag=0)\n",
    "        #create_graph(year, recs3, \"year\", \"rec\")\n",
    "\n",
    "        subjects,recs4 = get_tag(\"header/controlInfo/artinfo/sug/\", record, root, tag, G, flag =1)\n",
    "        #create_graph(subjects, recs4, \"subject\", \"rec\")\n",
    "\n",
    "        #place,recs5 = get_tag(\".//place\", record, root, tag, G, flag=2)\n",
    "        #create_graph(place, recs5, \"place\", \"rec\")\n",
    "\n",
    "\n",
    "        # list_of_lists = [authors, recs1, journals, recs2, year, recs3, subjects, recs4, place, recs5]\n",
    "        \n",
    "        \n",
    "        '''        \n",
    "\n",
    "        df2 = pd.DataFrame(journals, columns = ['node'])\n",
    "        #j_df = pd.DataFrame({'count' : df2.groupby(by='node').size().nlargest(25)}).reset_index()\n",
    "        for idx, row in df2.iterrows():\n",
    "            G.add_node(row['node'], type='journal')\n",
    "\n",
    "\n",
    "            \n",
    "        df5 = pd.DataFrame(place, columns = ['node'])\n",
    "        #p_df = pd.DataFrame({'count' : df5.groupby(by='node').size().nlargest(25)}).reset_index()\n",
    "        for idx, row in df5.iterrows():\n",
    "            G.add_node(row['node'], type='place')\n",
    "\n",
    "        \n",
    "        df3 = pd.DataFrame(year, columns = ['node'])\n",
    "        # y_df = pd.DataFrame({'count' : df3.groupby(by='node').size().nlargest(25)}).reset_index()\n",
    "        for idx, row in df3.iterrows():\n",
    "            G.add_node(row['node'], type='year')\n",
    "\n",
    "        \n",
    "        # create graph nodes for each of \n",
    "        df1 = pd.DataFrame(authors, columns = ['node'])\n",
    "        # au_df = pd.DataFrame({'count' : df1.groupby(by='node').size().nlargest(25)}).reset_index()\n",
    "        for idx, row in df1.iterrows():\n",
    "            G.add_node(row['node'], type='author')\n",
    "        '''\n",
    "        \n",
    "\n",
    "        df4 = pd.DataFrame(subjects, columns = ['node'])\n",
    "        # s_df = pd.DataFrame({'count' : df4.groupby(by='node').size().nlargest(25)}).reset_index()\n",
    "        for idx, row in df4.iterrows():\n",
    "            G.add_node(row['node'], type='subject')\n",
    "\n",
    "        #df_nodes = df4.copy(deep=True)\n",
    "        #df_nodes = pd.concat([df1, df3])\n",
    "        #df_nodes = pd.concat([df_nodes, df5])\n",
    "        #df_nodes = pd.concat([df_nodes, df4])\n",
    "        #df_nodes = pd.concat([df_nodes, df5])\n",
    "        #df_nodes.columns = ['node', 'count']\n",
    "        #print(df_nodes.head())\n",
    "\n",
    "        #df_nodes.head()\n",
    "    \n",
    "    #print(len(authors), len(journals), len(year), len(subjects), len(place))\n",
    "    #print(len(recs1), len(recs2), len(recs3), len(recs4), len(recs5))\n",
    "nx.write_gexf(G, 'graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b64192fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_df.head()\n",
    "len(tf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5b716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
